name: "b200-fp8-agg-mtp-1k8k"

model:
  path: "dsfp8"
  container: "lmsysorg/sglang:v0.5.6-cu129-amd64"
  precision: "fp8"

frontend:
  nginx_container: nginx-sqsh

resources:
  gpu_type: "b200"
  agg_nodes: 1
  agg_workers: 1
  gpus_per_node: 8

backend:
  aggregated_environment:
    TORCH_DISTRIBUTED_DEFAULT_TIMEOUT: "1800"
    PYTHONUNBUFFERED: "1"
    DYN_SKIP_SGLANG_LOG_FORMATTING: "1"
    SGLANG_ENABLE_JIT_DEEPGEMM: "false"
    SGLANG_ENABLE_FLASHINFER_GEMM: "1"
    SGLANG_DISABLE_TP_MEMORY_INBALANCE_CHECK: "1"
    NCCL_CUMEM_ENABLE: "1"
    SGLANG_ENABLE_SPEC_V2: "1"

  sglang_config:
    aggregated:
      # Model configuration
      served-model-name: "deepseek-ai/DeepSeek-R1"
      model-path: "/model/"
      trust-remote-code: true

      # Quantization and precision
      quantization: "fp8"
      kv-cache-dtype: "fp8_e4m3"

      # Parallelism
      tensor-parallel-size: 8
      data-parallel-size: 1
      expert-parallel-size: 1

      # Attention and MoE backends
      attention-backend: "trtllm_mla"
      moe-runner-backend: "flashinfer_trtllm"

      # Radix cache disabled
      disable-radix-cache: true

      # Streaming and timeout
      stream-interval: 30
      watchdog-timeout: 1000000

      # Memory and token limits
      mem-fraction-static: 0.82
      max-total-tokens: 32768
      max-prefill-tokens: 32768
      chunked-prefill-size: 32768

      # Batch and request limits
      cuda-graph-max-bs: 128
      max-running-requests: 128

      # Scheduler
      scheduler-recv-interval: 10
      enable-flashinfer-allreduce-fusion: true

      # MTP (Multi-Token Prediction) - EAGLE speculative decoding
      speculative-algorithm: "EAGLE"
      speculative-num-steps: 2
      speculative-eagle-topk: 1
      speculative-num-draft-tokens: 3

benchmark:
  type: "sa-bench"
  isl: 1024
  osl: 8192
  concurrencies: "4x8x16x32x64"
  req_rate: "inf"
