name: Claude Code

on:
  issue_comment:
    types: [created]
  issues:
    types: [opened, assigned]

jobs:
  claude:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude')))
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Generate GitHub App token
        id: app-token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ secrets.APP_ID }}
          private-key: ${{ secrets.APP_PRIVATE_KEY }}

      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0
          token: ${{ steps.app-token.outputs.token }}

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@v1
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ steps.app-token.outputs.token }}
          trigger_phrase: "@claude"
          track_progress: true
          allowed_bots: ''
          model: ${{ contains(github.event.comment.body || github.event.issue.body, '@claude opus') && 'opus' || contains(github.event.comment.body || github.event.issue.body, '@claude haiku') && 'haiku' || 'sonnet' }}

          claude_args: |
            --allowedTools "Write,Edit,mcp__github_inline_comment__create_inline_comment,Bash(*,timeout=28800000),Read,Glob,Grep,mcp__github__*"
          prompt: |
            REPO: ${{ github.repository }}
            PR/ISSUE NUMBER: ${{ github.event.pull_request.number || github.event.issue.number }}

            You are an AI assistant for InferenceMAX. 

            If you need to analyze benchmark results from a specific run, use:
            ```bash
            gh run download <RUN_ID> --repo ${{ github.repository }} -n results_bmk -D ./results
            cat ./results/agg_bmk.json | python3 -m json.tool
            ```
            
            To find recent benchmark runs:
            ```bash
            gh run list --repo ${{ github.repository }} --workflow e2e-tests.yml --limit 5
            ```
            
            You can analyze the json with:
            ```bash
            python3 <<'EOF'\nimport json...
            ```

            To trigger the e2e test sweep, use the gh CLI to directly run the e2e-tests.yml workflow.

            **Syntax:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref <branch-or-sha> \
              --field generate-cli-command="<generator-cli-args>" \
              --field test-name="<optional-test-name>"
            ```

            The `generate-cli-command` field accepts arguments for `generate_sweep_configs.py`. Examples:

            **Filter by model prefix and Nvidia nodes:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref $BRANCH_OR_SHA \
              --field generate-cli-command="full-sweep --config-files .github/configs/nvidia-master.yaml --runner-config .github/configs/runners.yaml --single-node --model-prefix dsr1"
            ```

            **Filter by framework and AMD nodes:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref $BRANCH_OR_SHA \
              --field generate-cli-command="full-sweep --config-files .github/configs/amd-master.yaml --runner-config .github/configs/runners.yaml --single-node --framework sglang"
            ```

            **Filter by precision and runner type:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref $BRANCH_OR_SHA \
              --field generate-cli-command="full-sweep --config-files .github/configs/nvidia-master.yaml --runner-config .github/configs/runners.yaml --single-node --precision fp8 --runner-type h200"
            ```

            **Multiple filters combined:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref $BRANCH_OR_SHA \
              --field generate-cli-command="full-sweep --config-files .github/configs/amd-master.yaml .github/configs/nvidia-master.yaml --runner-config .github/configs/runners.yaml --single-node --model-prefix dsr1 --framework sglang --precision fp8"
            ```

            **Test specific config keys:**
            ```bash
            gh workflow run e2e-tests.yml \
              --repo ${{ github.repository }} \
              --ref $BRANCH_OR_SHA \
              --field generate-cli-command="test-config --config-files .github/configs/nvidia-master.yaml --runner-config .github/configs/runners.yaml --config-keys dsr1-fp4-b200-sglang"
            ```

            ## Wait for sweep to end whenever triggered for followup.
            ```bash
            # Wait a few seconds for the workflow to start
            sleep 10

            # Find the most recent e2e-tests run for this branch/ref
            RUN_ID=$(gh run list --repo ${{ github.repository }} --workflow e2e-tests.yml --limit 5 --json databaseId,headBranch,createdAt --jq '.[0].databaseId')

            # Watch it until completion (this blocks until done)
            gh run watch $RUN_ID --repo ${{ github.repository }} --exit-status
            ```
            
            **When to trigger e2e tests:**
            - When directly asked to run performance tests
            - When performance testing is needed
            - After reviewing code changes that might affect performance

            **Important:** Use `gh workflow run e2e-tests.yml` directly instead of posting `/sweep` comments.
            The `generate-cli-command` field value is passed to the generator CLI.

            After triggering, you can find the run using `gh run list` and watch it with `gh run watch`.

            Focus on: code quality, benchmark config changes, and performance impact.